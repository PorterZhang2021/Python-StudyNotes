# 电影网站数据的爬取-Ajax+动态渲染

# 任务目标

爬取电影数据网站https://spa1.scrape.center/，此网站无反爬，数据通过Ajax加载，页面动态渲染，需要爬取的部分为列表页里面的电影数据详情。

# 任务目标解析

1. 爬取https://spa1.scrape.center/, 网站的列表页面，通过Ajax返回的api中的内容来获取我们所需要的内容信息
2. 爬取https://ssr1.scrape.center/detail/{id}, 网站内的数据详情，需要获取的部分有, 这里是直接采用的api进行返回,因此我们只需要获取api内的内容即可：
   1. 电影标题
   2. 电影图片的url
   3. 电影上映时间
   4. 电影分类
   5. 电影评分
   6. 剧情简介
3. 将内容存放到需要的数据库中

# 技术选型

## 基本分析

对于我们要进行爬取的内容，首先我们要对网页的相关逻辑进行分析，只有分析了网页的相关规律，才能够让我们更好的对内容进行爬取，我们首先知道我们要从索引页面即https://spa1.scrape.center/  中获取我们所需要的数据，通过打开调试工具栏中的network刷新后可以知道，如果我们要获取索引数据我们需要找出 https://spa1.scrape.center/api/movie/?limit={limit}&offset={offset} 的规律，而这里我们通过多个页面的规律发现，limit是不变的，一直为$10$,offset的规律则是 $(page - 1) * 10$，这样我们很容易就能够获取到所有的页面数据，接下来分析详情页的数据，我们发现 https://spa1.scrape.center/detail/1 的network响应的数据网址为 https://spa1.scrape.center/api/movie/{id}, 其中这里的id部分不断的变动，而这个id正好在索引页我们可以获取到，接下来就容易完成我们所需要解决的问题了。

## 如何爬取

### urllib库

利用urlllib库进行数据爬取主要的问题是获取的为api请求内容，当我们获取api当中请求内容时我们首先获取到的是一段json格式的数据，但这个数据如果我们要直接在python中使用的话，我们可能需要利用json库来将其进行格式的转化，最终就能够获取到我们所需要的网页json格式的数据了。

#### 抓取主函数

```python
def scrape_api(url):
    # 网页抓取
    logging.info('scraping %s ...', url)
    # 异常捕获
    try:
        # 响应
        response = urlopen(url)
        # 响应状态
        if response.status == 200:
            # 获取请求结果
            response_text = response.read()
            # json格式转字典
            response_result = json.loads(response_text)
            # 返回响应结果
            return response_result
        
        logging.error('get invalid status code %s while scraping %s',
                      response.status, url)
    except urllib.error.HTTPError as e:
        logging.error('error occured while scraping %s', url, exc_info=True)
```

#### 抓取索引页

```python
def scrape_index(page):
    index_url = INDEX_URL.format(limit=LIMIT, offset=(page - 1)*LIMIT)
    return scrape_api(index_url)
```

#### 抓取详情页

```python
def scrape_detail(id):
    detail_url = DETAIL_URL.format(id=id)
    # 获取详情页中的数据
    return scrape_api(detail_url)

```

### requests库

requests库解决此问题就比较容易了，它可以直接获取json格式的数据并将其转化成字典，因此当我们获取到响应后直接调用json()方法就能够直接在python中使用请求响应获取到的数据。

#### 抓取主函数

```python
def scrape_api(url: str) -> dict:
    """
        抓取api当中的内容并以字典格式进行返回
    """
    # 提示抓取某个网页
    logging.info('scraping %s ...', url)
    # 异常捕获
    try:
        # 响应获取
        response = requests.get(url=url)
        # 判断响应是不是200 ok
        if response.status_code == 200:
            # 返回内容为一个字典，这里是将json格式的内容
            # 转化了
            return response.json()
        # 如果不是返回响应的错误码
        logging.error('get an error for scraping %s, this error of status code is %s', url, response.status_code)
    except requests.RequestException as e:
        # 如果出现错误的响应请求直接返回错误信息
        logging.error('get an error about %s', e, exc_info=True)
```

#### 抓取索引页

```python
def scrape_index(page: int) -> dict:
    """
    获取到索引网站接口当中的内容，这里的url可以通过逻辑推断出来
    url中limit是恒定不变的10
    offset是(page - 1) * 10
    因此这里参数传入page即可
    """
    index_url = INDEX_URL.format(limit = 10, offset = (page - 1) * 10)
    return scrape_api(index_url)
```

#### 抓取详情页

```python
def scrape_detail(id: int) -> dict:
    """
    由列表页爬取后可以解析获取到id
    而id是我们所需要的获取ajax数据的内容，因此传入id即可
    这里返回的同样是dict格式
    """
    detail_url = DETAIL_URL.format(id=id)
    return scrape_api(detail_url)
```

### aiohttp库

利用aiohttp库进行数据抓取时，所采用的方式稍微有些不同，因为这个库是异步请求库，所以一次性可以进行多个异步请求并将其挂起，因此我们需要在每个函数的前面加上`async`关键字，至于请求后的json数据格式处理，这里直接使用json方法即可。

#### 抓取主函数

```python
async def scrape_api(url: str) -> dict:
    """
    传入参数：传入url链接
    任务目标：获取到请求的数据
    逻辑步骤：
    1. 构建异步请求的session
    2. 通过session来进行网页的请求访问
    3. 判断访问的的代码是不是200
        如果是200就返回json数据
    4. 不是的话就直接结束
    返回结果：最终返回结果最好为一个dict格式的对象
    """
    # 日志消息
    logging.info('scraping %s ...', url)
    # 异常捕获操作
    try:
        # 进行请求操作
        async with session.get(url=url) as response:
            # 查看请求后的状态
            if response.status == 200:
                # 返回dict对象
                return await response.json()
        # 返回错误消息
        logging.error('get an error of status code is %s while scraping %s ...',
                      response.status, url)
    except aiohttp.ClientError as e:
        logging.error('scraping %s ... has an error', url, exc_info=True)
```

#### 抓取索引页

```python
async def scrape_index(page: int) -> dict:
    """
    传入参数：page
    任务目标：获取索引页中的数据
    执行逻辑：
    1. 分析索引页的逻辑，构造url后传入抓取函数中
    这里分析结果limit不变为10，offset为(page-1) * 10满足条件
    2. 因此这里可以知道传入参数可以设置为page
    返回结果：最终结果应该为一个字典对象
    """
    # 索引链接
    index_url = INDEX_URL.format(limit=10, offset=(page - 1) * 10)
    # 返回响应后获取的数据
    return await scrape_api(url=index_url)
```

#### 抓取详情页

这里可以发现和上面几种写法不一样，这里获取到数据后我们就进行了写入，而上面几种则是获取到内容后在处理后进行写入，主要是因为await关键字后的对象具有特殊性，它需要满足下面三个条件之一：

1. 原生协程对象
2. `types.coroutine`修饰的生成器，这个生成器可以返回协程对象
3. 一个包含`__await__`方法的对象返回的一个迭代器

```python
# 详情页数据抓取
async def scrape_detail(id: int) -> dict:
    """
    传入参数: id
    任务目标：获取详情页中的数据
    执行逻辑：
    1. 传入id数据构建需要的url
    2. 利用抓取函数进行抓取
    返回结果：返回结果应该为一个字典对象
    """
    # 详情链接
    detail_url = DETAIL_URL.format(id=id)
    # 返回响应后获取的数据
    data =  await scrape_api(url=detail_url)
    # 异步写入
    await save_data(data=data)
```



### Selenium库



## 如何处理

### urllib库



### request库



### aiohttp库



### Selenium库



## 如何存储

### txt存储



### csv存储



### 异步txt存储



### mongoDB存储



# 附录-整体代码

## 利用urllib库进行数据的爬取



## 利用request库进行数据的爬取



## 利用aiohttp库进行异步数据的爬取



## 利用Selenium库进行电影网站数据的爬取

