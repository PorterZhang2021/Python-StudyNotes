# 利用requests库与lxml库进行电影网站数据的获取

# 任务目标

爬取电影数据网站https://ssr1.scrape.center/，此网站无反爬，数据通过服务端渲染，需要爬取的部分为列表页里面的电影数据详情。

# 任务目标解析

1. 爬取https://ssr1.scrape.center/网站的列表页面，通过列表页面的内容获取到需要的URL
2. 爬取https://ssr1.scrape.center/detail/{id}网站内的数据详情，需要获取的部分有：
   1. 电影标题
   2. 电影图片的url
   3. 电影上映时间
   4. 电影分类
   5. 电影评分
   6. 剧情简介
3. 将内容存放到需要的数据库中

# 技术选型与爬取

## 如何爬取

本次爬取选择使用在urllib库的基础上诞生的requests库，此库对urllib库的部分功能进行了重新的封装和扩展，使得网页爬取变得更加的容易。

### 构建基础的爬取函数

这部分的爬取函数的构建同之前使用urllib库构建的基本类似，但requests库对其进行了一些扩展，可以更好的做一些请求操作。

```python
def scrape_page(url):
    logging.info('scraping url: %s', url)
    # 进行异常操作
    try:
        response = requests.get(url=url)
        if response.status_code == 200:
            return response.text
        logging.error('get invalid status code %s while scraping %s',
                      response.status_code, url)
    except requests.RequestException:
        logging.error('error occured while scraping %s', url, exc_info=True)
```

### 构建列表页的爬取函数

在基础的爬取函数上进行相关内容的爬取，因此这里只需要更改url后直接调用爬取函数即可。

```python
# 爬取列表页
def scrape_index(page):
    index_url = f'{BASE_URL}/page/{page}'
    return scrape_page(index_url)
```

### 构建详情页的爬取函数

由于列表页解析后会获得所需要的url，因此我们直接调用url进行抓取页面即可。

```python
# 爬取详情页
def scrape_detail(url):
    return scrape_page(url)
```

## 如何解析

### xpath规则解析

本次使用的方式不再是使用最基本的正则表达式进行解析，虽然正则表达式在构建后能够准确的找到我们所需要的内容，但是构建一个正则表达式还是比较繁琐的，因此这里使用了xpath规则进行网页节点的提取，可以利用[xpath教程](https://www.w3school.com.cn/xpath/index.asp)简单快速的学习使用xpath。

### 解析列表页

在之前使用re模块进行列表解析时，我们需要构建一个正则表达式，实际上利用正则表达式，其实是对获取的网页进行了字符串匹配，因此当出现多个节点层没有办法找到容易辨识的特征时，构建正则表达式其实会比较麻烦，而使用xpath规则时就直接变得很简单，很容易就能够获取到我们需要的内容：

```python
# 解析列表页
def parse_index(html_str):
    html = etree.HTML(html_str)
    # 解析获取需要的内容
    items = html.xpath('//a[@class="name"]/@href')
    if not items:
        return []
    for item in items:
        # 进行链接的拼接
        detail_url = urljoin(BASE_URL, item)
        # 输出生成的链接信息
        logging.info('get detail url %s', detail_url)
        # 生成器函数
        yield detail_url
```

### 解析详情页

详情页的构建在之前需要多个正则表达式进行构建，而使用xpath规则时构建就容易许多，可以利用元素标签名+类名快速获取到我们所需要的内容，但re模块也不是没有特殊的用武之地，对于本次要抓取的电影网站来说，如果要获取到电影的上映时间，此时利用re模块获取速度能够特别的快。

```python
# 解析详情页
def parse_detail(html_str, index):
    html = etree.HTML(html_str)

    # 电影标题
    name = html.xpath('//h2[@class="m-b-sm"]/text()')[0] if html.xpath('//h2[@class="m-b-sm"]/text()') \
            else None
    # 电影图片URL
    cover = html.xpath('//img[@class="cover"]/@src')[0] if html.xpath('//img[@class="cover"]/@src') \
            else None
    # 电影上映时间
    published_at = re.compile('(\d{4}-\d{2}-\d{2}) 上映')
    published = re.search(published_at, html_str).group(1) if re.search(published_at, html_str) \
                else None
    # 电影分类
    categories = html.xpath('//button[contains(@class, "category")]/span/text()') \
                if html.xpath('//button[contains(@class, "category")]/span/text()') \
                else None
    # 电影评分
    score = html.xpath('//p[contains(@class, "score")]/text()')[0] \
            if html.xpath('//p[contains(@class, "score")]/text()') \
            else None
    # 电影剧情简介
    drama = html.xpath('//div[contains(@class, "drama")]/p/text()')[0] \
            if html.xpath('//div[contains(@class, "drama")]/p/text()') \
            else None
    
    # 以字典形式返回所需要的内容
    return {
        'index': index,
        'name': name,
        'cover': cover,
        'published': published,
        'categories': categories,
        'drama': drama.strip(),
        'score': score.strip()
    }
```

## 如何存储

本次存储使用了JSON格式的存储方式，由于我们获取数据时为字典，通过json模块能够十分便捷快速

的将数据存储，但是如果要在一个文件内存储，处理`[]`就不算太容易了，因此我们可以在完成此任务后手动添加。

```python
# 以json文件的方式进行存储
def save_data(data):
    data_path = '{0}/movies.json'.format(RESULT_DIR)

    # 创建一个进行内容的存放
    with open(data_path, 'a+', encoding='utf-8') as file:
        json.dump(data, file, ensure_ascii=False, indent=2)
        file.write(',')
        file.write('\n')
```

## 总结

利用request库与lxml库时，明显可以发现编码速率比之前快了很多，这就是一个成熟的工具库能够给我们带来的遍历，那么不禁要问我们为什么还要学习使用基本库呢？其实requests库在最初是对urllib库进行了一次封装，完善了一些功能的操作，弥补了urllib库的不足，后续随着不断的扩展延展达到了现在这样的程度，而我们学习基础库甚至这些成熟的工具库，更重要的是要读这些库的源码，学习高手的经验，通过对这些内容的学习，可以让我们脱离浅显层次的运用，不断的让我们进步。